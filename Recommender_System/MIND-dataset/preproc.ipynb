{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.9 MB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/giulia/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: requests in /Users/giulia/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/giulia/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.0-cp39-cp39-macosx_10_7_x86_64.whl (439 kB)\n",
      "\u001b[K     |████████████████████████████████| 439 kB 24.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp39-cp39-macosx_10_7_x86_64.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 27.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[K     |████████████████████████████████| 311 kB 47.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /Users/giulia/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/giulia/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/giulia/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: filelock in /Users/giulia/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[K     |████████████████████████████████| 166 kB 62.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/giulia/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/giulia/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/giulia/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/giulia/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/giulia/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/giulia/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Installing collected packages: fsspec, huggingface-hub, tokenizers, safetensors, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.2.0\n",
      "    Uninstalling fsspec-2022.2.0:\n",
      "      Successfully uninstalled fsspec-2022.2.0\n",
      "Successfully installed fsspec-2023.10.0 huggingface-hub-0.19.4 safetensors-0.4.0 tokenizers-0.15.0 transformers-4.35.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/giulia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/giulia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/giulia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/giulia/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "#disabe annoying warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101527, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_train = 'MINDlarge_train/news.tsv'\n",
    "news_test = 'MINDlarge_test/news.tsv'\n",
    "news_val = 'MINDlarge_val/news.tsv'\n",
    "\n",
    "#--------------------------------------------\n",
    "\n",
    "def load_news_df(path):\n",
    "    if 'news' in path:\n",
    "        columns = ['News ID',\n",
    "                \"Category\",\n",
    "                \"SubCategory\",\n",
    "                \"Title\",\n",
    "                \"Abstract\",\n",
    "                \"URL\",\n",
    "                \"Title Entities\",\n",
    "                \"Abstract Entities \"]\n",
    "    \n",
    "    elif 'behavior' in path:\n",
    "        columns = ['Impression ID',\n",
    "                \"User ID\",\n",
    "                \"Time\",\n",
    "                \"History\",\n",
    "                \"Impressions\"]\n",
    "    \n",
    "    df = pd.read_csv(path, sep='\\t', header=None, names=columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#--------------------------------------------\n",
    "\n",
    "news_df = load_news_df(news_train)\n",
    "news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96106, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove url and rows w/ missing values\n",
    "news_df = news_df.drop(columns=['URL'])\n",
    "news_df = news_df.dropna()\n",
    "news_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_df = news_df.copy()\n",
    "start_df = start_df.drop(columns=[\n",
    "                \"Title Entities\",\n",
    "                \"Abstract Entities \"])\n",
    "bert_df = start_df.copy()\n",
    "bert_df['combined_text'] = bert_df['Title'] + \" \" + bert_df['Abstract']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The Brands Queen Elizabeth, Prince Charles, an...\n",
       "1    Walmart Slashes Prices on Last-Generation iPad...\n",
       "2    50 Worst Habits For Belly Fat These seemingly ...\n",
       "4    The Cost of Trump's Aid Freeze in the Trenches...\n",
       "5    I Was An NBA Wife. Here's How It Affected My M...\n",
       "Name: combined_text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_df = bert_df[\"combined_text\"]\n",
    "bert_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>SubCategory</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>brand queen elizabeth prince charles prince ph...</td>\n",
       "      <td>shop notebook jacket royal cant live without</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N45436</td>\n",
       "      <td>news</td>\n",
       "      <td>newsscienceandtechnology</td>\n",
       "      <td>walmart slash price lastgeneration ipads</td>\n",
       "      <td>apple new ipad release bring big deal last yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N23144</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>worst habit belly fat</td>\n",
       "      <td>seemingly harmless habit holding back keeping ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N93187</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>cost trump aid freeze trench ukraine war</td>\n",
       "      <td>lt ivan molchanets peeked parapet sand bag fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>N75236</td>\n",
       "      <td>health</td>\n",
       "      <td>voices</td>\n",
       "      <td>nba wife here affected mental health</td>\n",
       "      <td>felt like fraud nba wife didnt help fact nearl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  News ID   Category               SubCategory  \\\n",
       "0  N88753  lifestyle           lifestyleroyals   \n",
       "1  N45436       news  newsscienceandtechnology   \n",
       "2  N23144     health                weightloss   \n",
       "4  N93187       news                 newsworld   \n",
       "5  N75236     health                    voices   \n",
       "\n",
       "                                               Title  \\\n",
       "0  brand queen elizabeth prince charles prince ph...   \n",
       "1           walmart slash price lastgeneration ipads   \n",
       "2                              worst habit belly fat   \n",
       "4           cost trump aid freeze trench ukraine war   \n",
       "5               nba wife here affected mental health   \n",
       "\n",
       "                                            Abstract  \n",
       "0       shop notebook jacket royal cant live without  \n",
       "1  apple new ipad release bring big deal last yea...  \n",
       "2  seemingly harmless habit holding back keeping ...  \n",
       "4  lt ivan molchanets peeked parapet sand bag fro...  \n",
       "5  felt like fraud nba wife didnt help fact nearl...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning & preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "  \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "    #string format\n",
    "    text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "start_df['Title'] = start_df['Title'].apply(preprocess_text)\n",
    "start_df['Abstract'] = start_df['Abstract'].apply(preprocess_text)\n",
    "#start_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News ID        0\n",
       "Category       0\n",
       "SubCategory    0\n",
       "Title          0\n",
       "Abstract       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "#validation set\n",
    "val_df = load_news_df(news_val)\n",
    "val_df = val_df.drop(columns=['URL', 'Title Entities', 'Abstract Entities '])\n",
    "val_df = val_df.dropna()\n",
    "val_df['Title'] = val_df['Title'].apply(preprocess_text)\n",
    "val_df['Abstract'] = val_df['Abstract'].apply(preprocess_text)\n",
    "#show number of missing values\n",
    "#missing_values = val_df.isnull().sum()  \n",
    "#missing_values #0 everywhere\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>SubCategory</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>brand queen elizabeth prince charles prince ph...</td>\n",
       "      <td>shop notebook jacket royal cant live without</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N23144</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>worst habit belly fat</td>\n",
       "      <td>seemingly harmless habit holding back keeping ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N93187</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>cost trump aid freeze trench ukraine war</td>\n",
       "      <td>lt ivan molchanets peeked parapet sand bag fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N75236</td>\n",
       "      <td>health</td>\n",
       "      <td>voices</td>\n",
       "      <td>nba wife here affected mental health</td>\n",
       "      <td>felt like fraud nba wife didnt help fact nearl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>N99744</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>get rid skin tag according dermatologist</td>\n",
       "      <td>seem harmless there good reason shouldnt ignor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  News ID   Category      SubCategory  \\\n",
       "0  N88753  lifestyle  lifestyleroyals   \n",
       "1  N23144     health       weightloss   \n",
       "3  N93187       news        newsworld   \n",
       "4  N75236     health           voices   \n",
       "5  N99744     health          medical   \n",
       "\n",
       "                                               Title  \\\n",
       "0  brand queen elizabeth prince charles prince ph...   \n",
       "1                              worst habit belly fat   \n",
       "3           cost trump aid freeze trench ukraine war   \n",
       "4               nba wife here affected mental health   \n",
       "5           get rid skin tag according dermatologist   \n",
       "\n",
       "                                            Abstract  \n",
       "0       shop notebook jacket royal cant live without  \n",
       "1  seemingly harmless habit holding back keeping ...  \n",
       "3  lt ivan molchanets peeked parapet sand bag fro...  \n",
       "4  felt like fraud nba wife didnt help fact nearl...  \n",
       "5  seem harmless there good reason shouldnt ignor...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extraction: \n",
    "tf-idf<br>\n",
    "NOTA: look into SF-IDF aka using wordnet synonym to replace tf-idf. Problem: are extracted token semantically meaningfull enough??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aac', 'aaron', 'aaron boone', 'aaron hick', 'aaron rodgers', 'ab', 'abandoned', 'abbott', 'abc', 'abc news', 'ability', 'able', 'able get', 'aboard', 'abortion']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "num_tokens = 15 #to print\n",
    "\n",
    "with open('tfidf_matrix.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    first_row = next(reader)\n",
    "\n",
    "\n",
    "first_few_tokens = first_row[1:num_tokens + 1] \n",
    "print(first_few_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf train set\n",
    "start_df['combined_text'] = start_df['Title'] + \" \" + start_df['Abstract']\n",
    "tfidf_matrix = tfidf.fit_transform(start_df['combined_text'])\n",
    "\n",
    "# tf-idf save matrix as .csv\n",
    "save_path = 'tfidf_matrix.csv'\n",
    "pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names()).to_csv(save_path, index=False)\n",
    "# takes 7m 54s \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# validation set\n",
    "val_df['combined_text'] = val_df['Title'] + \" \" + val_df['Abstract']\n",
    "val_tfidf_matrix = tfidf.fit_transform(val_df['combined_text'])\n",
    "pd.DataFrame(val_tfidf_matrix.toarray(), columns=tfidf.get_feature_names()).to_csv('val_tfidf_matrix.csv', index=False)\n",
    "#takes 5m 37s\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train word2vec model\n",
    "tokenized_text = [text.split() for text in start_df['combined_text']]\n",
    "word2vec_model = Word2Vec(tokenized_text, vector_size=300, window=5, min_count=2, workers=4) #how to chose best hyperparameters?\n",
    "\n",
    "#save model as .csv\n",
    "pd.DataFrame(word2vec_model.wv.vectors).to_csv('word2vec_model.csv', index=False)\n",
    "#25sec ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation word2vec model\n",
    "val_tokenized_text = [text.split() for text in val_df['combined_text']]\n",
    "word2vec_model = Word2Vec(val_tokenized_text, vector_size=300, window=5, min_count=2, workers=4)\n",
    "pd.DataFrame(word2vec_model.wv.vectors).to_csv('val_word2vec_model.csv', index=False)\n",
    "#20sec ?????"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.backends.mps.is_available())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [tokenizer(text, padding=True, truncation=True, return_tensors='pt') for text in bert_df['combined_text']] #return pytorch tensors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
