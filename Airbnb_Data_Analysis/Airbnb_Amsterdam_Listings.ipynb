{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AirBnb Amsterdam Listings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disable some annoying warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "#plots the figures in place instead of a new window\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats \n",
    "\n",
    "from sklearn.cluster import KMeans, Birch, MiniBatchKMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mtl\n",
    "import altair as alt\n",
    "import seaborn as sns\n",
    "\n",
    "from ipywidgets import interact  \n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "alt.data_transformers.disable_max_rows()\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "# figure sizes\n",
    "standard_fig_size = (21,6)\n",
    "boxplot_fig_size = (15,3)\n",
    "heatmap_fig_size = (24,12)\n",
    "countplot_fig_size = (15,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a standard dataset of heterogenous data\n",
    "listings = pd.read_csv('data/listings.csv')\n",
    "listings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping superflous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the features that are not important for us.\n",
    "misc_features = [\n",
    "    'listing_url',\n",
    "    'scrape_id',\n",
    "    'name',\n",
    "    'description',\n",
    "    'neighborhood_overview',\n",
    "    'picture_url',\n",
    "    'host_url',\n",
    "    'host_location',\n",
    "    'host_about',\n",
    "    'host_thumbnail_url',\n",
    "    'host_picture_url',\n",
    "    'neighbourhood',\n",
    "    'neighbourhood_group_cleansed',\n",
    "    # 'latitude',\n",
    "    # 'longitude',\n",
    "    'calendar_updated',\n",
    "    'calendar_last_scraped',\n",
    "    'license'\n",
    "]\n",
    "\n",
    "# dropping the columns from df\n",
    "listings = listings.drop(misc_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data adjustments\n",
    "\n",
    "For example the list of 'host_verification' options is not that important, what is important is how many different options the host provides/requires for verification.\n",
    "\n",
    "Features to adjust:\n",
    "* bathroom_text, ditch text (only number is useful), move then to column bathrooms\n",
    "* host_verification as an amount of options\n",
    "* amenities as an amount of options\n",
    "* districts, 7 districts form 26 neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile('[a-z A-Z]')\n",
    "\n",
    "# doing some float adjustemnts\n",
    "listings['bathrooms'] = listings['bathrooms_text'].fillna('NaN').str.replace(regex, r\"\\0\", regex=True).str.strip('\\x00 -').astype('string').replace('', 'NaN').astype('float64')\n",
    "listings['host_verifications'] = listings['host_verifications'].map(lambda x: len([_x.strip() for _x in eval(x) ]))\n",
    "listings['amenities'] = listings['amenities'].map(lambda x: len([_x.strip() for _x in eval(x)])) \n",
    "listings['price'] = listings['price'].astype('string').str.strip('$').str.replace(',', '').astype('float64')\n",
    "listings['host_response_rate'] = listings['host_response_rate'].str.replace('%', '').astype('float64')\n",
    "listings['host_acceptance_rate'] = listings['host_acceptance_rate'].str.replace('%', '').astype('float64')\n",
    "\n",
    "# doing some data adjustments\n",
    "# --> use dataype 'timedelta' to have the datatype as a reference between two datetimes\n",
    "listings['last_scraped'] = listings['last_scraped'].astype('datetime64')\n",
    "listings['host_since'] = listings['host_since'].astype('datetime64')\n",
    "listings['first_review'] = listings['first_review'].astype('datetime64')\n",
    "listings['last_review'] = listings['last_review'].astype('datetime64')\n",
    "\n",
    "neighbourhood_district = {\n",
    "    'Centre District': ['Centrum-Oost', 'Centrum-West'],\n",
    "    'Nieuw-West District': ['Geuzenveld - Slotermeer', 'Slotervaart', 'De Aker - Nieuw Sloten', 'Osdorp'],\n",
    "    'Noord District': ['Noord-Oost', 'Noord-West', 'Oud-Noord'],\n",
    "    'Oost District': ['Oostelijk Havengebied - Indische Buurt', 'IJburg - Zeeburgereiland', 'Oud-Oost', 'Watergraafsmeer'],\n",
    "    'West District': ['De Baarsjes - Oud-West', 'Westerpark', 'Bos en Lommer'],\n",
    "    'Zuid District': ['Buitenveldert - Zuidas', 'De Pijp - Rivierenbuurt', 'Zuid'],\n",
    "    'Zuidoost District': ['Bijlmer-Centrum', 'Gaasperdam - Driemond', 'Bijlmer-Oost']\n",
    "}\n",
    "\n",
    "d = {k: oldk for oldk, oldv in neighbourhood_district.items() for k in oldv}\n",
    "\n",
    "listings['neighbourhood_district'] = listings['neighbourhood_cleansed'].map(d)\n",
    "# drops\n",
    "listings = listings.drop('bathrooms_text', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper look into missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure setup fro following figures\n",
    "sns.set(rc={'figure.figsize':heatmap_fig_size})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a deeper understanding of data gathering and the issues that come with missing data, a heatmap displaying null/NaN values is very powerful to find them fast and maybe eliminate some of the features, not only for data visualization and interpretation but also for the possible future training of an agent on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(listings.isnull(), yticklabels=False, cbar=False, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also observe that the bathrooms, bedrooms and beds feature (all discrete/categorical variables) have sparse null values in them, we can now just fill these null values with the mean value\n",
    "in order to have a full dataset where no entries have to be dropped. For features like host_neighborhood where there are alot on null values we can consider dropping the feature, but \n",
    "due to the fact that we are not training an ML agent it is not that important to have a very clean dataset. Maybe we can find something interesiting regarding missing host_neighborhood values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings['bathrooms'] = listings['bathrooms'].fillna(round(listings['bathrooms'].mean())) # interger or half\n",
    "listings['bedrooms'] = listings['bedrooms'].fillna(round(listings['bedrooms'].mean())) # integer\n",
    "listings['beds'] = listings['beds'].fillna(round(listings['beds'].mean())) # integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more look on the clean data to see if we missed something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(listings.isnull(), yticklabels=False, cbar=False, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics on individual Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure setup for following figures\n",
    "sns.set(rc={'figure.figsize':boxplot_fig_size})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will have a look at the *price* feature which is a continous feature.\n",
    "\n",
    "Starting with a boxplot of the price to get a grip of the summary statistics of the price attribute, the mean is marked as a white dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(\n",
    "    listings['price'], \n",
    "    showmeans=True,\n",
    "    meanprops={\n",
    "        \"marker\":\"o\",\n",
    "        \"markerfacecolor\":\"white\", \n",
    "        \"markeredgecolor\":\"black\",\n",
    "        \"markersize\":\"6\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is one outlier that is siginificantly bigger then the rest, so our mean is affected by this to a certain extent that we will investigate later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this has to be the biggest outlier\n",
    "print(f\"Biggest Outlier: {listings['price'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inorder to get a better look at the boxplot, we  plot it without the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(\n",
    "    listings['price'],\n",
    "    showfliers=False,\n",
    "    showmeans=True,\n",
    "    meanprops={\n",
    "        \"marker\":\"o\",\n",
    "        \"markerfacecolor\":\"white\", \n",
    "        \"markeredgecolor\":\"black\",\n",
    "        \"markersize\":\"6\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can immediately see that the mean is pulled to the right (higher values) of the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the summary statistics needed for the boxplot not in a plot but in raw numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_min = listings['price'].min()\n",
    "q1 = listings['price'].quantile(0.25)\n",
    "median = listings['price'].quantile(0.5)\n",
    "q3 = listings['price'].quantile(0.75)\n",
    "_max = listings['price'].max()\n",
    "\n",
    "print(f\"Minimum: {_min}\")\n",
    "print(f\"Quartile 1: {q1}\")\n",
    "print(f\"Quartile 2/Median: {median}\")\n",
    "print(f\"Quartile 3: {q3}\")\n",
    "print(f\"Maximum: {_max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see from the boxplot that there is indeed a (or mulitple) listings with price 0, this is more or less an issue, so we'll have a short look into said listings/entries to determine if this effects more then one column and possibly replace these (preferable with the median, because the mean is so greatly effected by the big outlier, so it would not be representative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings[listings['price'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to these Datasets not having a lot of representation (7 entries out of 5402) we can delete them without having a loss of representation and have a better look at the price boxplot again. Further some of the data entries are very sparse populated and not useful for any other furter usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = listings.drop(listings[listings['price'] == 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(\n",
    "    listings['price'],\n",
    "    showfliers=False,\n",
    "    showmeans=True,\n",
    "    meanprops={\n",
    "        \"marker\":\"o\",\n",
    "        \"markerfacecolor\":\"white\", \n",
    "        \"markeredgecolor\":\"black\",\n",
    "        \"markersize\":\"6\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_min = listings['price'].min()\n",
    "q1 = listings['price'].quantile(0.25)\n",
    "median = listings['price'].quantile(0.5)\n",
    "q3 = listings['price'].quantile(0.75)\n",
    "_max = listings['price'].max()\n",
    "\n",
    "print(f\"Minimum: {_min}\")\n",
    "print(f\"Quartile 1: {q1}\")\n",
    "print(f\"Quartile 2/Median: {median}\")\n",
    "print(f\"Quartile 3: {q3}\")\n",
    "print(f\"Maximum: {_max}\")\n",
    "print(f\"Interquartile Range: {q3 - q1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a realistic minimum of 9 (Euros). My guess for the 0 Euro price is that these rooms were from Hotels (as one can guess from the feature room_type) or the name of the listing. They probably wanted to promote the hotel rooms somehow and did not set a price for the listing to lure the user onto their website instead of booking the room via AirBnb. As also already described, the data in some of these listings is pretty sparse!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution and Summary Statistics of price feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure setup for following figures\n",
    "sns.set(rc={'figure.figsize':standard_fig_size})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a look into the distribution of the price feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "prices = listings.copy()['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Skewness, the measure of asymmetry (skew) of a normal or probability distribution, and Kurtosis, the shape, height and steepness of a normal or probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Skewness: {prices.skew()}\")\n",
    "print(f\"Kurtosis: {prices.kurtosis()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the skewness value of $22.12$ we can see that the data is skewed (really really) heavily to the left and from the kurotisis value of $816.78$ one can conclude that the \n",
    "peak is very high and steep. Following these observations one can also conclude that the data is most likely (kind of) skewed normally distributed.\n",
    "\n",
    "Plotting the probability distribution of the *price*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    prices, \n",
    "    kind='kde',     \n",
    "    height=5,\n",
    "    aspect=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the price is a (kinda wobbly) skewed normally distribution is also backed by the plot of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the mean, variance, median and mode so we can see how the data is effected, rather poluted, by outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean: {prices.mean()}\")\n",
    "print(f\"Variance: {prices.var()}\")\n",
    "print(f\"Median: {prices.median()}\")\n",
    "print(f\"Mode: {prices.mode()[0]} (Value), {len(prices[prices == prices.mode()[0]])} /  {len(prices[prices == prices.mode()[0]])/prices.count()*100:.2f}% (Occurences)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the mean one can see a pretty representative value, altough it is biased by the rather big values (what one can kind of confirm by the noticeably smaller median). The variance suffers a little bit more from this, which also leads to the quick assumption that the data is pretty wide spread, but it is not what we can \n",
    "see from the distribution plot above, where the density peaks around the mean/median and therefore very low in the spectrum, which leads us to the conclusion that \n",
    "the data is indeed not very wide spread, despite the variance suggesting so, but rather concentrated at around 50 to 300. Looking at the mode we see that we have a value of $150$ and an occurence count of $225$ / $4.17\\%$ which is rather a lot if one considers the price as an arbitrarly chooseable number in $5395$ data entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = sns.histplot(prices, bins=100)\n",
    "_ = graph.set_xticks([300*i for i in range(100)])\n",
    "graph.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot confirms the belief that the data is in fact mostly centered around 50 to 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution and Summary Statistics of price feature w/o outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a look into the distribution of the price feature but only considering prices < 1000 to account for the majority, the really big, of outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = listings.copy()[listings['price'] < 1000]['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again some summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Mean: {prices.mean()}\")\n",
    "print(f\"Variance: {prices.var()}\")\n",
    "print(f\"Median: {prices.median()}\")\n",
    "print(f\"Mode: {prices.mode()[0]} (Value), {len(prices[prices == prices.mode()[0]])} / {len(prices[prices == prices.mode()[0]])/prices.count()*100:.2f}% (Occurences)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The mean still is far away from the median, which tells us that there are still (much bigger) outliers left in the data, this spread is also backed by the still high variance (altough it dropped by half). Obviously the data is still concentrated around 50 to 300. Median and Mode stayed unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=boxplot_fig_size)\n",
    "\n",
    "sns.boxplot(\n",
    "    prices,\n",
    "    showfliers=False,\n",
    "    showmeans=True,\n",
    "    meanprops={\n",
    "        \"marker\":\"o\",\n",
    "        \"markerfacecolor\":\"white\", \n",
    "        \"markeredgecolor\":\"black\",\n",
    "        \"markersize\":\"6\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot visually confirmes that the mean is still pulled to the right, but not as heavy as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Skewness and Kurtosis for Distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Skewness: {prices.skew()}\")\n",
    "print(f\"Kurtosis: {prices.kurtosis()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One observes that all of a sudden the skewness and kurtosis drop significantly to a skewness value of $2.51$ and a kurtosis value of $10.22$. The data is still highly skewed to the right (value above +1) and the peak (kurtosis) is still high and steep, but both values are nearly not as extreme as before. Still one can conclude that the data is kinda skewed normally distributed, but now much nicer for sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    prices, \n",
    "    kind='kde',     \n",
    "    height=5,\n",
    "    aspect=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution plot still shows that the *price* is a (much cleaner and not that extremely) skewed (kinda wobbly) normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the mean, variance and median of the cleansed prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = sns.histplot(prices, bins=100)\n",
    "_ = graph.set_xticks([300*i for i in range(100)])\n",
    "graph.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot confirms the belief that the data is in fact mostly centered around 50 to 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive Distribution and Boxplot of price feature (upper bound and boxplot outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can play around with the upper bound of the price feature to plot a distribution plot and some summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider\n",
    "\n",
    "@interact(\n",
    "    cutoff=IntSlider(\n",
    "        min=0, \n",
    "        max=listings['price'].max() + 10, \n",
    "        step=10,\n",
    "        description=\"Price upper bound: \",\n",
    "        continous_update=True\n",
    "    ),\n",
    "    outliers=widgets.Checkbox(\n",
    "        value=True,\n",
    "        description='Show outliers in Boxplot',\n",
    "        disabled=False,\n",
    "        indent=False\n",
    "    )\n",
    ")\n",
    "def _(cutoff, outliers):\n",
    "    prices = listings[listings['price'] < cutoff]['price']\n",
    "        \n",
    "    if prices.count() != 0:\n",
    "        occurences = prices[prices == prices.mode()[0]].count()\n",
    "        mode = prices.mode()[0]\n",
    "    else:\n",
    "        occurences = 'nan'\n",
    "        mode = 'nan'\n",
    "    \n",
    "    print('-----------------------')\n",
    "    print(f\"| Mean: {prices.mean():.3f}\")\n",
    "    print(f\"| Variance: {prices.var():.3f}\")\n",
    "    print(f\"| Median: {prices.median():.3f}\")\n",
    "    print(f\"| Mode: {mode} (Value), {occurences} (Occurences)\")    \n",
    "    print('-----------------------')\n",
    "    print(f\"| Skewness: {prices.skew():.3f}\")\n",
    "    print(f\"| Kurtosis: {prices.kurtosis():.3f}\")\n",
    "    print('-----------------------')\n",
    "    \n",
    "    sns.displot(\n",
    "        prices, \n",
    "        kind='kde',     \n",
    "        height=5,\n",
    "        aspect=4\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    sns.boxplot(\n",
    "        prices,\n",
    "        showfliers=outliers,\n",
    "        showmeans=True,\n",
    "        meanprops={\n",
    "        \"marker\":\"o\",\n",
    "        \"markerfacecolor\":\"white\", \n",
    "        \"markeredgecolor\":\"black\",\n",
    "        \"markersize\":\"6\"\n",
    "        }\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bathrooms/Bedrooms/Beds Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three of these features (bathrooms, bedrooms, beds) are basically Categorical/Discrete features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure setup for following figures\n",
    "sns.set(rc={'figure.figsize':countplot_fig_size})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are some helper functions to make everything nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Plotting a countplot for given feature. \n",
    "      - Ascending has to be set to False in case of a string feature\n",
    "\"\"\"\n",
    "def countplot(keyword: str, ascending=True):\n",
    "\n",
    "    graph = sns.countplot(\n",
    "        x=keyword,\n",
    "        data=listings\n",
    "    )\n",
    "\n",
    "    counts = listings[keyword].value_counts().sort_index(ascending=ascending).to_numpy()\n",
    "    \n",
    "    for i, p in enumerate(graph.patches):\n",
    "        height = p.get_height()\n",
    "        graph.text(p.get_x() + p.get_width() / 2., height + 0.1, f\"{(counts[i]/listings[keyword].count())*100:.1f}%\", ha=\"center\")\n",
    "        \n",
    "\"\"\"\n",
    "    Plotting a countplot for given (multiple) features in one plot. \n",
    "\"\"\"\n",
    "def countplot_multiple(keywords: [str], ascending=True):\n",
    "    graph = sns.countplot(\n",
    "        x='value',\n",
    "        hue='variable',\n",
    "        data=pd.melt(listings[keywords])\n",
    "    )\n",
    "    graph.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next are some countplots inorder to get a grip of the distribution of the categorical/discrete features of bathrooms, bedrooms and beds using the occurence counts of the individual categorical/discrete values.\n",
    "\n",
    "Due to the fact that bed and bedrooms share the same scale one can draw them togehter in a single countplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countplot_multiple(['beds', 'bedrooms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot does not hold that much statistical information, rather information about their correlation. To briefly sum it up: They (obviously) correlate with each other, altough there are a lot more single bedroom apartments then apartmnts with a single bed, i'll leave it at that, the point of this section is not to find correlations, thus the next plots are also countplots with a percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Countplot of *bathrooms*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countplot('bathrooms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was kinda predictable that the most listings ($60.9\\%$) would be single bedrooms, because even in a three bedroom apartment it is ethically reasonable to have a single bathroom. What is interesting is that $0.6\\%$ of the listings have 0 bathrooms, this could be just 0 values, so the host didn't bother with putting the info in or there is some kind of shared bathroom stuff going on, which is rather unlikely. Besides that the amount of listings (except for 0 bathrooms) drops off exponentially with encreasing steps/bathrooms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Countplot of *beds*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countplot('beds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again nearly about half ($48.6\\%$) of the listings are single bed apartments, after that again the count/percentage is dropping off exponentially. Remark: there are apartemnts with e.g 24 or 33 beds, but due to rounding and displaying the plot nicely the percentage is $0.0\\%$ and obviously the bar is not visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Countplot of *beds*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countplot('bedrooms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than half ($58.6$%) of the listings are single bedroom apartments, after that again the count/percentage is dropping off exponentially. Remark: there are apartemnts with e.g 7 bedrooms, but due to rounding and displaying the plot nicely the percentage is $0.0$% and obviously the bar is not visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Times Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short look at the categroical feature of response times.\n",
    "\n",
    "From the heatmap seen in section one, one can observe that here one finds a lot of missing values, just as a remark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure setup for following figures\n",
    "sns.set(rc={'figure.figsize':countplot_fig_size})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again a countplot is the plot to go for categorical features like *host_response_time*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countplot('host_response_time', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that in $59.4\\%$ of the time the host resonds \"within an hour\", \"responding within a few hours\" and \"within a day\" are with $19.9\\%$ and $18.7\\%$ respectively pretty close, this might be due to the classification process e.g. the mean of response times falls into the category \"within a few hours\" between 2 and 5 hours as *host_response_time*. Further the subjective time difference between immediately and 1 hour is not that much less then e.g. between 2 hours annd 5 hours, in the latter the time streches much longer from an absolute point of view, but in a subjective one this could be, at least for some, not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the features this is referring to are continous features.\n",
    "\n",
    "These Features all analyzed in this section span:\n",
    "* review_scores_rating\n",
    "* review_scores_accuracy\n",
    "* review_scores_cleanliness\n",
    "* review_scores_checkin\n",
    "* review_scores_communication\n",
    "* review_scores_location\n",
    "* review_scores_value\n",
    "\n",
    "Here again one can observe a lot of missing values seen in the boxplot in section 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure setup for following figures\n",
    "sns.set(rc={'figure.figsize':standard_fig_size})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function and helper variable to make everything nicer and easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a variable for convenience\n",
    "rating_features = [\n",
    "    'review_scores_rating', \n",
    "    'review_scores_accuracy', \n",
    "    'review_scores_cleanliness', \n",
    "    'review_scores_checkin', \n",
    "    'review_scores_communication', \n",
    "    'review_scores_location', \n",
    "    'review_scores_value'\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "    Plotting a boxplot for given (multiple) features in one plot. \n",
    "\"\"\"\n",
    "def boxplot_multiple(keywords: [str], outliers=True):\n",
    "    graph = sns.boxplot(\n",
    "        y='value',\n",
    "        x='variable',\n",
    "        data=pd.melt(listings[keywords]),\n",
    "        showfliers=outliers,\n",
    "        showmeans=True,\n",
    "        meanprops={\n",
    "            \"marker\":\"o\",\n",
    "            \"markerfacecolor\":\"white\", \n",
    "            \"markeredgecolor\":\"black\",\n",
    "            \"markersize\":\"6\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First looking at the continous data as a boxplot, again here with the mean as a white dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_multiple(rating_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can only observe that all the different review features are concentrated in the top values (4 to 5), looking at the boxplots without outliers one might get a better look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_multiple(rating_features, outliers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here one can see clearly now that the outliers pull the mean below the median, some stronger then others, for example the mean of *review_scores_value* is further at the median as for *review_scores_cleanliness*, 4 out of the 7 have their maximum value as quartile1, 2 of theses 4 have a very high median, but a mean close to quartile2 and therefore the data is concentrated at the very very top. The boxplots for *review_scores_cleanliness* and *review_scores_location* here look pretty equal except for the mean, which is higher in the *review_scores_location*, which is due to the outliers one can observe in the plot above. The only boxplot beeing different then all the others is the one for *review_scores_value* with generally way lower values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the skewness and kurtosis of the features, maybe the data is somehow normally distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rf in rating_features:\n",
    "    print(f\"{rf}:\")\n",
    "    print(f\"    Skewness: {listings[rf].skew()}\")\n",
    "    print(f\"    Kurtosis: {listings[rf].kurtosis()}\")\n",
    "    print(f\"    Mean: {listings[rf].mean()}\")\n",
    "    print(f\"    Variance: {listings[rf].var()}\")\n",
    "    print(f\"    Median: {listings[rf].median()}\")\n",
    "    print(f\"    Mode: {listings[rf].mode()[0]} (Value), {len(listings[listings[rf] == listings[rf].mode()[0]])} / {len(listings[listings[rf] == listings[rf].mode()[0]])/listings[rf].count()*100:.2f}% (Occurences)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These summary statistics, skew and kurtosis do not let one observe something out of the ordinary. The review features generally have a rather low variance, meaning that the data is closely distributed around the mean. The review features also have a general negative skewness to variying degrees, meaning all the feature's distributions are skewed to the right, altough the kurtosis varies more then the skewness, meaning some distributions are lower and flatter then others. Altough what is interesting is that the mode is a very common (in terms of amount) value and the maximum value, for example the mode of $5.0$ is $40.39\\%$ of the values for *review_scores_communication*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution Plot of all the review features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    x='value',\n",
    "    hue='variable',\n",
    "    data=pd.melt(listings[listings[rating_features] > 2.5][rating_features]),\n",
    "    kind='kde',\n",
    "    height=5,\n",
    "    aspect=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can again see what was prophesied by the skewness and kurtosis. The Distributions commly look very woobly with a small hill at $4.0$. The one that already stood out in the boxplots, *review_scores_value*, has a stranger distribution then the rest with its peak at around $4.7$ and a small hill at $5.0$. Looking at *review_scores_checkin* one can see that it is way less wobbly then the rest and peaking second highest at nearly $5.0$. The feature *review_scores_communication* peaks highest, also its boxplot looked pretty one sided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations between Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an overview of how the attributes in the dataset are correlated, a correlation heatmap might be of great help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set figure size\n",
    "plt.figure(figsize=(45, 35))\n",
    "# define plot with appropriate colormap\n",
    "heatmap = sns.heatmap(listings.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "# set title of heatmap\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':35}, pad=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obvious relationships can already be seen for the different types of reviews, furnishing and listing counts (greenish blocks).\n",
    "Correlations with higher positive and negative values seem to be appropriate to focus on (excluding values close to _-1_ or _1_ as they are mostly obvious)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations of Neighbourhood / Room-type Distributions / Rating / Price\n",
    "For now, the focus will be on correlations between the different _neighbourhoods_ and the corresponding _location-ratings_, _prices_ and _room-type_ distributions. Taking a look into the distribution of the _neighbourhoods_ and _room-types_ in general will give a good intuition for further correlations and/or findings.\n",
    "#### (short introduction) Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and print room_types\n",
    "total_all = len(listings.index)\n",
    "homes_all = len(listings[listings['room_type']=='Entire home/apt'].index)\n",
    "private_rooms_all = len(listings[listings['room_type']=='Private room'].index)\n",
    "shared_rooms_all = len(listings[listings['room_type']=='Shared room'].index)\n",
    "hotel_rooms_all = len(listings[listings['room_type']=='Hotel room'].index)\n",
    "\n",
    "#print(f'Total listings: {total_all}\\nHomes: {homes_all}\\nPrivate rooms: {private_rooms_all}\\nShared rooms: {shared_rooms_all}\\nHotel rooms: {hotel_rooms_all}')\n",
    "# Plotting room type distribution\n",
    "_ = plt.figure(figsize=(12, 7))\n",
    "room_type_distribution = plt.bar([f\"Entire Homes/Apt\\nn = {homes_all}\", f\"Private Rooms\\nn = {private_rooms_all}\", f\"Shared Rooms\\nn = {shared_rooms_all}\", f\"Hotel Rooms\\nn = {hotel_rooms_all}\"], [homes_all, private_rooms_all, shared_rooms_all, hotel_rooms_all], color=[\"blue\", \"pink\", \"green\", \"red\"])\n",
    "_ = plt.ylabel(\"n listings\", fontsize = 16)\n",
    "_ = plt.title(f'Room type distribution\\nTotal Listings: {total_all}', fontsize = 18)\n",
    "_ = plt.xticks(fontsize=14)\n",
    "_ = plt.yticks(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than half of the listings are **entire homes/apartments**. **Private rooms** are about 1/3 of the listings. Interestingly, **shared rooms** and **hotel rooms** make up only a really small amount in listings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Get neighbourhood counts\n",
    "listings_neighbourhood = listings[\"neighbourhood_cleansed\"].groupby(listings[\"neighbourhood_cleansed\"].tolist()).size()\n",
    "for index, neighb in enumerate(listings_neighbourhood.index):\n",
    "    ## better formatting\n",
    "    if len(neighb) == 6:\n",
    "        tabs = \"\\t\\t\\t\\t\\t\\t\"\n",
    "    elif len(neighb) == 4:\n",
    "        tabs = \"\\t\\t\\t\\t\\t\\t\"\n",
    "    elif len(neighb) < 15:\n",
    "        tabs = \"\\t\\t\\t\\t\\t\"\n",
    "    elif len(neighb) < 23:\n",
    "        tabs = \"\\t\\t\\t\\t\"\n",
    "    elif len(neighb) > 30:\n",
    "        tabs = \"\\t\\t\"\n",
    "    else:\n",
    "        tabs = \"\\t\\t\\t\"   \n",
    "    #print(f'{neighb}:{tabs}{listings_neighbourhood[index]}')\n",
    "#print()\n",
    "#print(f'Total amount of listings: {listings_neighbourhood.sum()}')\n",
    "\n",
    "def plot_nd():\n",
    "    # Figsize\n",
    "    _ = plt.figure(figsize=(20, 15))\n",
    "\n",
    "    # Get colormap\n",
    "    cmap = plt.get_cmap(\"tab20\")\n",
    "    color = cmap([i*2 if i < 11 else i*2 + 1 for i in range(10)])\n",
    "\n",
    "    ## Plotting\n",
    "    neighbourhood_distribution = plt.bar(listings_neighbourhood.index, listings_neighbourhood, color=color)\n",
    "\n",
    "    ## Title and Labels\n",
    "    _ = plt.ylabel(\"n listings\", fontsize = 18)\n",
    "    _ = plt.title(f'Neighbourhood distribution\\nTotal Listings: {total_all}\\n', fontsize = 18)\n",
    "    \n",
    "    for p in neighbourhood_distribution.patches:\n",
    "        width = p.get_width()\n",
    "        height = p.get_height()\n",
    "        percentage = height*100/total_all\n",
    "        x, y = p.get_xy()\n",
    "        plt.annotate(f'{percentage:.1f}%', (x + width/2, y + height*1.02), ha='center', fontsize=15)\n",
    "\n",
    "    # Rotate labels to be readable\n",
    "    xticks_pos = [0.65*patch.get_width() + patch.get_xy()[0] for patch in neighbourhood_distribution]\n",
    "    _ = plt.xticks(xticks_pos, listings_neighbourhood.index,  ha='right', rotation=45, fontsize = 14)\n",
    "    \n",
    "    return neighbourhood_distribution, color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood_distribution, color = plot_nd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the _neighbourhoods_ one can clearly see a trend where most of the listings are located at. Namely **Centrum-Oost**, **Centrum-West**, **De Baarsjes - Oud-West** and **De Pijp - Rivierenbuurt** have the highest amount of listings.  Some striking low numbers of listings are recognizable in some few neighbourhoods. These might be interesting to investigate further for corresponding _ratings_ and _prices_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neighbourhood / Room-type distributions\n",
    "The next chapter focuses on the _room-type_ distributions for each neighbourhood, visualized in a respective histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get individual neighbourhood-data\n",
    "\n",
    "total = []\n",
    "homes = []\n",
    "private_rooms = []\n",
    "shared_rooms = []\n",
    "hotel_rooms = []\n",
    "\n",
    "for i in listings_neighbourhood.index:\n",
    "    listings_current_neighbourhood = listings[listings[\"neighbourhood_cleansed\"] == i]\n",
    "    \n",
    "    total.append(len(listings_current_neighbourhood.index))\n",
    "    homes.append(len(listings_current_neighbourhood[listings_current_neighbourhood['room_type']=='Entire home/apt'].index))\n",
    "    private_rooms.append(len(listings_current_neighbourhood[listings_current_neighbourhood['room_type']=='Private room'].index))\n",
    "    shared_rooms.append(len(listings_current_neighbourhood[listings_current_neighbourhood['room_type']=='Shared room'].index))\n",
    "    hotel_rooms.append(len(listings_current_neighbourhood[listings_current_neighbourhood['room_type']=='Hotel room'].index))\n",
    "    \n",
    "      \n",
    "labels = listings_neighbourhood.index\n",
    "\n",
    "### Bar plot\n",
    "# figsize\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "\n",
    "# the label locations\n",
    "x = np.arange(len(labels))\n",
    "# the width of the bars\n",
    "width = 0.20  \n",
    "\n",
    "# 4 different subplots for room types\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, homes, width, label='Homes / Apt', color=['pink'])\n",
    "rects2 = ax.bar(x + width/2, private_rooms, width, label='Private rooms', color=['blue'])\n",
    "rects3 = ax.bar(x - width - width/2, shared_rooms, width, label='Shared rooms', color=['green'])\n",
    "rects4 = ax.bar(x + width + width/2, hotel_rooms, width, label='Hotel rooms', color=['red'])\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('n listings', fontsize=16)\n",
    "ax.set_title(f'Room type Distribution by Neighbourhood\\n', fontsize = 20)\n",
    "ax.legend()\n",
    "\n",
    "# Rotate labels to be readable\n",
    "xticks_pos = [0.65*patch.get_width() + patch.get_xy()[0] for patch in neighbourhood_distribution]\n",
    "_ = plt.xticks(xticks_pos, listings_neighbourhood.index,  ha='right', rotation=45, fontsize = 12)\n",
    "\n",
    "# Set layout\n",
    "_ = fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# reset figsize for further plots \n",
    "plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest amount of homes is found in **De Baarsjes - Oud-West**, whereas **Centrum-West** has the highest number of private rooms. Notably, even though both neighbourhoods have a pretty similar amount of listings, their room type distribution is very different.\n",
    "\n",
    "When only focusing on _neighbourhoods_ with more than 300 listings, **Centrum-West** and **Centrum West** have by far the highest distributions of **private rooms**.\n",
    "\n",
    "To see if there are any clear correlations and to get a good overview, the next two chapters will visualize neighbourhood ratings and price differences.\n",
    "\n",
    "#### Neighbourhood / Rating\n",
    "As there are a few entries with price values equal to zero and some few entries with a notably high price, dropping corresponding outliers for the following plots gives a better picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get individual neighbourhood data for plotting\n",
    "\n",
    "neighbourhood_ratings = []\n",
    "neighbourhood_prices = []\n",
    "\n",
    "\n",
    "for i in listings_neighbourhood.index:\n",
    "    listings_current_neighbourhood = listings[listings[\"neighbourhood_cleansed\"] == i]\n",
    "    \n",
    "    neighbourhood_prices.append(pd.DataFrame(listings_current_neighbourhood[\"price\"].tolist(), columns=[i]))\n",
    "    # drop entries where price == 0 or > 1000\n",
    "    neighbourhood_prices[-1] = neighbourhood_prices[-1][neighbourhood_prices[-1]!=0]\n",
    "    neighbourhood_prices[-1] = neighbourhood_prices[-1][neighbourhood_prices[-1]<1000]\n",
    "    \n",
    "    neighbourhood_ratings.append(pd.DataFrame(listings_current_neighbourhood[\"review_scores_location\"].tolist(), columns=[i]))    \n",
    "    \n",
    "neighbourhood_prices_df = pd.concat(neighbourhood_prices, ignore_index=True)\n",
    "neighbourhood_ratings_df = pd.concat(neighbourhood_ratings, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To directly compare the rating-distribution over all neighbourhoods, a density-plot seems to be a good start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plot = sns.displot(\n",
    "    data=neighbourhood_ratings_df,\n",
    "    kind='kde',\n",
    "    height=8,\n",
    "    aspect=2,\n",
    "    palette=color\n",
    ")\n",
    "_ = sns.set(font_scale = 2)\n",
    "_ = plot.set(xlabel='Location rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The density plot already highlights 4 different neighbourhoods which have the majority of their _ratings_ mostly centered around 5. A further boxplot will let one compare the different areas much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot6(outliers = False):\n",
    "    # Plotting\n",
    "    plt.rcParams[\"figure.figsize\"] = (16,10)\n",
    "\n",
    "    graph = sns.boxplot(\n",
    "        showfliers=outliers,\n",
    "        data=neighbourhood_ratings_df,\n",
    "        palette=color)\n",
    "    \n",
    "    if outliers:\n",
    "        graph.axes.set_title(\"Location Rating by Neighbourhood\\nwith outliers\\n\",fontsize=20)\n",
    "    else:\n",
    "        graph.axes.set_title(\"Location Rating by Neighbourhood\\n\",fontsize=20)\n",
    "    graph.set_ylabel(\"Location rating  [0-5]\",fontsize=16)\n",
    "    graph.tick_params(labelsize=12)\n",
    "\n",
    "    # Rotate labels to be readable\n",
    "    xticks_pos = [0.65*patch.get_width() + patch.get_xy()[0] for patch in neighbourhood_distribution]\n",
    "    _ = plt.xticks(xticks_pos, listings_neighbourhood.index,  ha='right', rotation=45)\n",
    "\n",
    "    _ = plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]\n",
    "\n",
    "plot6(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot6(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, **Centrum-Oost** and **Centrum-West** are the highest rated locations. This also makes sense when looking at the amount of listings in the corresponding areas. Especially the latter, **Centrum-West**, seems to be the highest rating neighbourhood. **De Pijp - Rivierenbuurt** is worth mentioning as well.\n",
    "\n",
    "Furthermore, the ratings for **Gaasperdam-Driemond** vary the most - it has the lowest boxplot-minimum while also having its maximum at 5. Remarkably, the lowest median values are found in **Bijlmer-Oost** and **Osdorp**. It may be noted that these three neighbourhoods only have a small amount of listings available, which certainly could be the reason for their extraordinary numbers.\n",
    "\n",
    "#### Neighbourhood / Price\n",
    "The focus now switches to the relationship between the neighbourhoods and their respective price distributions. The approach stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = sns.displot(\n",
    "    data=neighbourhood_prices_df,\n",
    "    kind='kde',\n",
    "    height=7,\n",
    "    aspect=3,\n",
    "    palette=color\n",
    ")\n",
    "sns.set(font_scale = 4)\n",
    "_ = plot.set(xlabel='Price in ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The density plot already indicates, that most locations have similar prices on the lower end. A few neighbourhoods have remarkably larger price ranges than the remaining locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot7(outliers = False):\n",
    "    # Plotting\n",
    "    plt.rcParams[\"figure.figsize\"] = (17,12)\n",
    "    graph = sns.boxplot(\n",
    "        showfliers=outliers,\n",
    "        data=neighbourhood_prices_df,\n",
    "        palette=color)\n",
    "    \n",
    "    if outliers:\n",
    "        graph.axes.set_title(\"Price by Neighbourhood\\nwith outliers\\n\",fontsize=20)\n",
    "    else:\n",
    "        graph.axes.set_title(\"Price by Neighbourhood\\n\",fontsize=20)\n",
    "        \n",
    "    graph.set_ylabel(\"Price in \",fontsize=16)\n",
    "    graph.tick_params(labelsize=12)\n",
    "\n",
    "    # Rotate labels to be readable\n",
    "    xticks_pos = [0.65*patch.get_width() + patch.get_xy()[0] for patch in neighbourhood_distribution]\n",
    "    _ = plt.xticks(xticks_pos, listings_neighbourhood.index,  ha='right', rotation=45)\n",
    "    _ = plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]\n",
    "\n",
    "plot7(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot7(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the price-distribution, it seems obvious that there are many outliers present. Focusing on the plot with hidden outliers should give a good enough picture about the different distributions. The **minimum-prices** of the neighbourhoods **vary between 10-50**. The **maximum-prices** of the neighbourhoods vary between **200-420**\n",
    "\n",
    "**Gaasperdam - Driemond** has the lowest and also smallest price range, even though its neighbourhood has the highest variation in its location ratings. Again, it may be noted that only 0.8% of listings are located there.\n",
    "\n",
    "**Ijburg - Zeeburgereiland** reaches the highest maximum at about **420** with a large price range, while having an average number of listings available. Its rating is situated rather average.\n",
    "**Centrum--Oost** and **Centrum-West** are very similar, but with much more listings and higher ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion / Correlations\n",
    "To see if there is any relationship between the _location-rating_ and _price_ for each neighbourhood, plotting it as histogramms collectively might already highlight any correlation. The _ratings_ and _prices_ will be averaged for each neighbourhood.\n",
    "\n",
    "The average _rating_ values vary between 4 and 5, whereas the average _prices_ vary between approx. 100 and 180. When plotting on the same graph, one needs to \"norm\" respective y-axes. \n",
    "\n",
    "Furthermore, to highlight neighbourhoods with good _ratings_ and low _prices_, a stronger color opacity will indicate respective areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get individual neighbourhood-data\n",
    "\n",
    "prices = []\n",
    "ratings = []\n",
    "\n",
    "\n",
    "for i in listings_neighbourhood.index:\n",
    "    listings_current_neighbourhood = listings[listings[\"neighbourhood_cleansed\"] == i]\n",
    "    \n",
    "    prices.append(listings_current_neighbourhood[\"price\"])\n",
    "    # drop entries where price == 0 or > 500\n",
    "    prices[-1] = prices[-1][prices[-1]!=0]\n",
    "    prices[-1] = prices[-1][prices[-1]<500]\n",
    "    \n",
    "    prices[-1] = prices[-1].mean()\n",
    "    \n",
    "    ratings.append(listings_current_neighbourhood[\"review_scores_location\"])\n",
    "    ratings[-1] = ratings[-1].mean()\n",
    "    \n",
    "    \n",
    "      \n",
    "labels = listings_neighbourhood.index\n",
    "\n",
    "### calculations to center the mean values\n",
    "min_price = min(prices)\n",
    "max_price = max(prices)\n",
    "\n",
    "min_rating = min(ratings)\n",
    "max_rating = max(ratings)\n",
    "\n",
    "avg_price = sum(prices)/len(prices)\n",
    "avg_rating = sum(ratings)/len(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot1\n",
    "def plot1(figsize = (15,10)):\n",
    "    # Figsize\n",
    "    _ = plt.figure(figsize=figsize)\n",
    "\n",
    "    # Get colormap\n",
    "    cmap = plt.get_cmap(\"Blues\")\n",
    "    color = cmap(0.5)\n",
    "    \n",
    "    ## Plotting\n",
    "    neighbourhood_distribution = plt.bar(labels, prices, color=color, width=0.35)\n",
    "\n",
    "    ## Title and Labels\n",
    "    _ = plt.ylabel(\"Average Price in \", fontsize=16)\n",
    "    _ = plt.title(f'Average Price by Neighbourhood\\n', fontsize=20)\n",
    "    _ = plt.yticks(fontsize=14)\n",
    "    \n",
    "    # Set background and grid color\n",
    "    ax.set_facecolor('0.9')\n",
    "    ax.grid(color='0.85', linewidth=0.7)\n",
    "\n",
    "    # Rotate labels to be readable\n",
    "    xticks_pos = [0.65*patch.get_width() + patch.get_xy()[0] for patch in neighbourhood_distribution]\n",
    "    _ = plt.xticks(xticks_pos, listings_neighbourhood.index,  ha='right', rotation=45, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot2\n",
    "def plot2(figsize = (15,10)):\n",
    "    # Figsize\n",
    "    _ = plt.figure(figsize=figsize)\n",
    "\n",
    "    # Get colormap\n",
    "    cmap = plt.get_cmap(\"Greens\")\n",
    "    color = cmap(0.5)\n",
    "    \n",
    "    ## Plotting\n",
    "    neighbourhood_distribution = plt.bar(labels, ratings, color=color, width=0.35)\n",
    "\n",
    "    ## Title and Labels\n",
    "    _ = plt.ylabel(\"Average Location Rating [0-5]\", fontsize=16)\n",
    "    _ = plt.title(f'Average Rating by Neighbourhood\\n', fontsize=20)\n",
    "    _ = plt.yticks(fontsize=14)\n",
    "    \n",
    "    # Set background and grid color\n",
    "    ax.set_facecolor('0.9')\n",
    "    ax.grid(color='0.85', linewidth=0.7)\n",
    "\n",
    "    # Rotate labels to be readable\n",
    "    xticks_pos = [0.65*patch.get_width() + patch.get_xy()[0] for patch in neighbourhood_distribution]\n",
    "    _ = plt.xticks(xticks_pos, listings_neighbourhood.index,  ha='right', rotation=45, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot3\n",
    "def plot3(figsize = (15,10)):\n",
    "    \n",
    "    ### Bar plot\n",
    "    # figsize\n",
    "    plt.rcParams[\"figure.figsize\"] = figsize\n",
    "\n",
    "    # the label locations\n",
    "    x = np.arange(len(labels))\n",
    "    # the width of the bars\n",
    "    width = 0.275\n",
    "\n",
    "    # 2 different subplots for Price and Rating\n",
    "    fig, ax = plt.subplots()\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    # Colors\n",
    "    # We want to distinguish low prices with high ratings\n",
    "\n",
    "    # get individual colormaps\n",
    "    cmap_price = mtl.cm.get_cmap('Blues')\n",
    "    cmap_rating = mtl.cm.get_cmap('Greens')\n",
    "\n",
    "    # get individual color-values as list of colors\n",
    "    colors_price = cmap_price(0.5)\n",
    "    colors_rating = cmap_rating(0.5)\n",
    "\n",
    "    rects1 = ax.bar(x - width/2, prices, width, color=colors_price)\n",
    "    rects2 = ax2.bar(x + width/2, ratings, width, color=colors_rating)\n",
    "\n",
    "    ax.set_ylim(bottom=0, top=max_price + 0.1*avg_price)\n",
    "    ax2.set_ylim(bottom=0, top=max_rating + 0.1*avg_rating)\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    legend = plt.legend([rects1, rects2],[\"Price\", \"Rating\"], loc=\"upper left\", prop={'size': 15})\n",
    "\n",
    "    ax.set_title(f'Average Price/Rating by Neighbourhood\\n', fontsize=20)\n",
    "    price_label = ax.set_ylabel(\"Average Price in \", fontsize=16)\n",
    "    price_label.set_color(cmap_price(0.6))\n",
    "    rating_label = ax2.set_ylabel(\"Average Location Rating [0-5]\", fontsize=16)\n",
    "    rating_label.set_color(cmap_rating(0.6))\n",
    "\n",
    "    # Rotate labels to be readable\n",
    "    xticks_pos = [0.65*patch.get_width() + patch.get_xy()[0] for patch in neighbourhood_distribution]\n",
    "    _ = ax.set_xticks(xticks_pos, listings_neighbourhood.index,  ha='right', rotation=45, fontsize=12)\n",
    "    _ = ax2.set_xticks(xticks_pos, listings_neighbourhood.index,  ha='right', rotation=45, fontsize=12)\n",
    "    \n",
    "    _ = plt.yticks(fontsize=14)\n",
    "\n",
    "    # Set background and grid color\n",
    "    ax.set_facecolor('0.95')\n",
    "    ax2.set_facecolor('0.95')\n",
    "    ax.grid(color='0.85', linewidth=0.7)\n",
    "    ax2.grid(color='0.85', linewidth=0.7)\n",
    "    \n",
    "    ax.yaxis.set_tick_params(labelsize=14)\n",
    "    ax2.yaxis.set_tick_params(labelsize=14)\n",
    "\n",
    "    # Set layout\n",
    "    _ = fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # reset figsize for further plots \n",
    "    plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot4(figsize = (15,10)):\n",
    "    ### Bar plot\n",
    "    # figsize\n",
    "    plt.rcParams[\"figure.figsize\"] = figsize\n",
    "\n",
    "    # the label locations\n",
    "    x = np.arange(len(labels))\n",
    "    # the width of the bars\n",
    "    width = 0.3  \n",
    "\n",
    "    # 2 different subplots for Price and Rating\n",
    "    fig, ax = plt.subplots()\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    # Colors\n",
    "    # We want to distinguish low prices with high ratings\n",
    "\n",
    "    # get individual colormaps\n",
    "    cmap_price = mtl.cm.get_cmap('Blues')\n",
    "    cmap_rating = mtl.cm.get_cmap('Greens')\n",
    "\n",
    "    # norm values from 0 - 1\n",
    "    norm_price = mtl.colors.Normalize(vmin=min_price, vmax=max_price)\n",
    "    norm_rating = mtl.colors.Normalize(vmin=min_rating, vmax=max_rating)\n",
    "    price_normalized = norm_price(prices)\n",
    "    rating_normalized = norm_rating(ratings)\n",
    "\n",
    "    # get comparision value, new intervall [0, 2]\n",
    "    comp_values = [1 + rating_normalized[i] - price for i, price in enumerate(price_normalized)]\n",
    "\n",
    "    # best_value, if rating is high and price is low\n",
    "    best_value = max(comp_values)\n",
    "    # worst_value, if rating is low and price is high\n",
    "    worst_value = min(comp_values)\n",
    "    # avg_value for checking (by looking at the dataset, should be centered around 1)\n",
    "    avg_value = sum(comp_values)/len(comp_values)\n",
    "    if 0.9 > avg_value or 1.1 < avg_value:\n",
    "        print(\"Error in calculation\")\n",
    "    \n",
    "\n",
    "    # set min value and max value with equal offset (we do not want white or black colors)\n",
    "    norm_values = mtl.colors.Normalize(vmin=worst_value*0.4, vmax=2-worst_value*0.6)\n",
    "\n",
    "    # normalize again to be between 0 and 1 for colormaps\n",
    "    color_values = norm_values(comp_values)\n",
    "\n",
    "    # check again\n",
    "    if 0.9 > sum(comp_values)/len(comp_values) or 1.1 < sum(comp_values)/len(comp_values):\n",
    "        print(\"Error in calculation\")\n",
    "\n",
    "    # get individual color-values as list of colors\n",
    "    colors_price = cmap_price(color_values)\n",
    "    colors_rating = cmap_rating(color_values)\n",
    "\n",
    "    rects1 = ax.bar(x - width/2, prices, width, color=colors_price)\n",
    "    rects2 = ax2.bar(x + width/2, ratings, width, color=colors_rating)\n",
    "\n",
    "    ax.set_ylim(bottom=min_price - 0.03*avg_price, top=max_price + 0.03*avg_price)\n",
    "    ax2.set_ylim(bottom=min_rating - 0.03*avg_rating, top=max_rating + 0.03*avg_rating)\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    legend = plt.legend([rects1, rects2],[\"Price\", \"Rating\"], loc=\"upper left\", prop={'size': 15})\n",
    "\n",
    "    ax.set_title(f'Average Price/Rating by Neighbourhood\\n', fontsize=20)\n",
    "    price_label = ax.set_ylabel(\"Average Price in \", fontsize=16)\n",
    "    price_label.set_color(cmap_price(0.6))\n",
    "    rating_label = ax2.set_ylabel(\"Average Location Rating [0-5]\", fontsize=16)\n",
    "    rating_label.set_color(cmap_rating(0.6))\n",
    "\n",
    "    # Rotate labels to be readable\n",
    "    xticks_pos = [0.65*patch.get_width() + patch.get_xy()[0] for patch in neighbourhood_distribution]\n",
    "    _ = ax.set_xticks(xticks_pos, labels,  ha='right', rotation=45, fontsize=12)\n",
    "    _ = ax2.set_xticks(xticks_pos, labels,  ha='right', rotation=45, fontsize=12)\n",
    "    \n",
    "    # Set background and grid color\n",
    "    ax.set_facecolor('0.96')\n",
    "    ax2.set_facecolor('0.96')\n",
    "    ax.grid(color='0.85', linewidth=0.7)\n",
    "    ax2.grid(color='0.85', linewidth=0.7)\n",
    "    \n",
    "    ax.yaxis.get_label().set_fontsize(14)\n",
    "    ax2.yaxis.get_label().set_fontsize(14)\n",
    "    \n",
    "    ax.yaxis.set_tick_params(labelsize=14)\n",
    "    ax2.yaxis.set_tick_params(labelsize=14)\n",
    "    \n",
    "    # Set layout\n",
    "    _ = fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # reset figsize for further plots \n",
    "    plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figsize = (15, 8)\n",
    "figsize2 = (15, 12)\n",
    "plot1(figsize)\n",
    "plot2(figsize)\n",
    "plot3(figsize2)\n",
    "plot4(figsize2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already seen in previous plots and now highlighted by darker colors, **Gaasperdam - Driemond** and **Slotervaart** have pretty high rating-vales while also maintaining low prices.\n",
    "\n",
    "In contrast, **Ijburg - Zeeburgereiland** has pretty high avg. prices and a relatively low avg-rating.\n",
    "\n",
    "As most neighbourhoods seem to be around the same spectrum, plotting the same graph again ordered by prices may yield a nice correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get individual neighbourhood-data ORDERED\n",
    "\n",
    "_prices = []\n",
    "ratings = []\n",
    "\n",
    "\n",
    "for i in listings_neighbourhood.index:\n",
    "    listings_current_neighbourhood = listings[listings[\"neighbourhood_cleansed\"] == i]\n",
    "    \n",
    "    _prices.append(listings_current_neighbourhood[\"price\"])\n",
    "    # drop entries where price == 0 or > 500\n",
    "    _prices[-1] = _prices[-1][_prices[-1]!=0]\n",
    "    _prices[-1] = _prices[-1][_prices[-1]<500]\n",
    "    \n",
    "    _prices[-1] = _prices[-1].mean()\n",
    "    \n",
    "    ratings.append(listings_current_neighbourhood[\"review_scores_location\"])\n",
    "    ratings[-1] = ratings[-1].mean()\n",
    "    \n",
    "labels = list(listings_neighbourhood.index)\n",
    "    \n",
    "prices, ratings = (list(t) for t in zip(*sorted(zip(_prices, ratings))))\n",
    "\n",
    "idx = np.argsort(_prices)\n",
    "\n",
    "_, labels = (list(t) for t in zip(*sorted(zip(_prices, labels))))\n",
    "\n",
    "### calculations to center the mean values\n",
    "min_price = min(prices)\n",
    "max_price = max(prices)\n",
    "\n",
    "min_rating = min(ratings)\n",
    "max_rating = max(ratings)\n",
    "\n",
    "avg_price = sum(prices)/len(prices)\n",
    "avg_rating = sum(ratings)/len(ratings)\n",
    "\n",
    "plot4(figsize2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the best _price/rating_ scores can be found for the cheapest neighbourhoods. Centered neighbourhoods maintain an average _price/rating_, whereas the worst scoring neighbourhoods are on the more expensive(right hand) side.\n",
    "\n",
    "One can clearly see its correlation, as there are genereally higher ratings on the right hand side.\n",
    "\n",
    "**Note**: The best _price/rating_ score is reached, when a neighbourhood has the highest (relative) _rating_ and the lowest (relative) _price_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation: Response-rate/Review-scores for superhosts and non-superhosts\n",
    "For the next chapter, the difference between **superhosts** and **non-superhosts** will be analyzed. By looking at the qualifications for superhosts on https://www.airbnb.com/help/article/829/how-to-become-a-superhost, superhosts have to maintain a higher _response-rate_ than 90% and an _overall-rating_ higher than 4.8.\n",
    "\n",
    "To get an overview on how many **superhosts** and **non-superhosts** are in the database, their corresponding amount is printed and their respective correlation plots will be plotted further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print counts\n",
    "n_superhosts = listings['host_is_superhost'].value_counts()[1]\n",
    "n_nsuperhosts = listings['host_is_superhost'].value_counts()[0]\n",
    "print(f'Number of superhosts: {n_superhosts}\\nNumber of non-superhosts: {n_nsuperhosts}')\n",
    "\n",
    "# split dataset into two separate datasets\n",
    "is_sh = listings['host_is_superhost'] == \"t\"\n",
    "is_nsh = listings['host_is_superhost'] == \"f\"\n",
    "listings_sh = listings[is_sh]\n",
    "listings_nsh = listings[is_nsh]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are more than twice as many non-superhosts compared to superhosts, reducing the transparency of non-superhost entries will yield a more readable scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot5(regression = False):\n",
    "    fig, ax = plt.subplots(figsize=(17,10))\n",
    "\n",
    "    # Plotting of scatter-plot for superhosts and non-superhosts\n",
    "    \n",
    "    if not(regression):\n",
    "        _ = plt.scatter('host_response_rate', 'review_scores_rating', c = [\"green\"], data=listings_sh)\n",
    "        _ = plt.scatter('host_response_rate', 'review_scores_rating', c = [\"red\"], data=listings_nsh, alpha = 0.2)\n",
    "    else:\n",
    "        _ = plt.scatter('host_response_rate', 'review_scores_rating', c = [\"red\"], data=listings)\n",
    "    \n",
    "    if not(regression):\n",
    "        _ = ax.set_title(f'Response rate / Review scores scatterplot\\n', fontsize=20)\n",
    "    else:\n",
    "        _ = ax.set_title(f'Response rate / Review scores scatterplot\\n with 3rd order regression line\\n', fontsize=20)\n",
    "    \n",
    "    if regression:\n",
    "        sns.regplot(x='host_response_rate', y='review_scores_rating', data=listings, scatter=False, order=3)\n",
    "    \n",
    "    if not(regression):\n",
    "        # Legend\n",
    "        lgnd = plt.legend(['is superhost', 'is not a superhost'], loc='lower center', bbox_to_anchor=(-0.12, 0.1, 0.5, 0.5), prop={'size': 15})\n",
    "        colors=['green', 'red']\n",
    "        for i, j in enumerate(lgnd.legendHandles):\n",
    "            j.set_color(colors[i])\n",
    "            j.set_alpha(1)\n",
    "    _ = plt.ylabel(\"Review scores rating  [0-5]\", fontsize=15)\n",
    "    _ = plt.xlabel(\"% Host response rate\", fontsize=15)\n",
    "    \n",
    "    ax.xaxis.set_tick_params(labelsize=14)\n",
    "    ax.yaxis.set_tick_params(labelsize=14)\n",
    "    \n",
    "plot5(True)\n",
    "plot5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are extrems on both ends. Clearly, **superhosts** generally maintain a higher _response-rate_ and _ratings_. Yet when looking at the plot, one can detect some **superhosts** that should not have the qualifications to be one. \n",
    "\n",
    "Furthermore, what seems unusual are the number of hosts that have a _response-rate_ of 0% and exactly 50%. Also interesting to interpret are the corresponding _ratings_ that are mostly higher than 4. This might be an indication that the dataset's rating values are not scraped accurately for all the listings.\n",
    "\n",
    "With the help of the **blue regression line** one can validate its correlation.\n",
    "\n",
    "To confirm that there are indeed **unqualified superhosts** present in the dataset, the total number of **superhosts** that have either a _response-rate_ < 90% or a _rating_ < 4.8 are calculated, printed and plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_invalid_sh = listings_sh[(listings_sh[\"review_scores_rating\"] < 4.8) | (listings_sh[\"host_response_rate\"] < 90)].count()[0]\n",
    "print(f'Number of superhosts: {listings_sh.count()[0]}\\nNumber of unqualified superhosts: {number_of_invalid_sh}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately **17% of superhosts** should not be eligible to be one. This might indicate that airbnb does not enforce their own superhost-qualifications as written on their site. The following plot additionally confirms the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into seperate datasets\n",
    "invalid_sh = listings_sh[(listings_sh[\"review_scores_rating\"] < 4.8) | (listings_sh[\"host_response_rate\"] < 90)]\n",
    "valid_sh = listings_sh[(listings_sh[\"review_scores_rating\"] >= 4.8) | (listings_sh[\"host_response_rate\"] >= 90)]\n",
    "\n",
    "# Plotting\n",
    "_ = plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "_ = plt.ylabel(\"Review scores rating  [0-5]\", fontsize=16)\n",
    "_ = plt.xlabel(\"% Host response rate\", fontsize=16)\n",
    "_ = plt.title(f'Response rate / Review scores scatterplot\\n', fontsize=20)\n",
    "\n",
    "plot = plt.scatter('host_response_rate', 'review_scores_rating', c = [\"green\"], data=valid_sh)\n",
    "plot = plt.scatter('host_response_rate', 'review_scores_rating', c = [\"red\"], data=invalid_sh)\n",
    "# Legend\n",
    "lgnd = plt.legend(['valid superhost', 'invalid superhost'], loc='lower center', bbox_to_anchor=(-0.12, 0.1, 0.5, 0.5), prop={'size': 15})\n",
    "colors=['green', 'red']\n",
    "for i, j in enumerate(lgnd.legendHandles):\n",
    "    j.set_color(colors[i])\n",
    "    j.set_alpha(1)\n",
    "\n",
    "# horizontal and vertical lines\n",
    "ynew = 4.8\n",
    "xnew = 90\n",
    "plt.axhline(ynew, color='b')\n",
    "plt.axvline(xnew, color='b')\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Reset figsize for further plots\n",
    "_ = plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, the number of listings with a _response-rate_ of 0% and 50% shrunk immensely. Still, one listing remained with a _response-rate_ of 0%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation: Host-verifications vs Number-of-Reviews/Availability\n",
    "\n",
    "Hosts on airbnb can have different kind of verifications. The details of what specific verification a host has aquired have been omitted and only the amount of verifications has been counted. When looking at the **correlation-plot**, one can clearly see that there is a:\n",
    "\n",
    "- **positive correlation** for _host-verifications_ and _number-of-reviews_\n",
    "- **negative correlation** for _host-verifications_ and _availability_\n",
    "\n",
    "This might indicate, that hosts with more verifications get booked more often and therefore yield a higher amount of reviews while having a lower availabilty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the focus is on a correlation between one categorical and one continuos attribute, a multi-boxplot will give a good overview to inspect the links between the two attributes. Furthermore, since the **sample sizes** for each number of _host-verifications_ might invalidate some bins, they will simultanously be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset splitup into n host verification lists\n",
    "\n",
    "number_of_reviews = []\n",
    "host_ver_count = []\n",
    "columns = []\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,7)\n",
    "\n",
    "print()\n",
    "for i in range(listings[\"host_verifications\"].max()):\n",
    "    host_ver = listings[listings[\"host_verifications\"] == i]\n",
    "    host_ver_count.append(host_ver[\"availability_365\"].count())\n",
    "    number_of_reviews.append(pd.DataFrame(host_ver[\"number_of_reviews\"].tolist(), columns=[i]))\n",
    "    columns.append(i)\n",
    "    \n",
    "df_number_of_reviews = pd.concat(number_of_reviews, ignore_index=True)\n",
    "\n",
    "ax = sns.countplot(data = df_number_of_reviews, color='b')\n",
    "_ = ax.set_title(f'Countplot \\'host_verifications\\'\\n', fontsize=20)\n",
    "_ = ax.set_xlabel('n host verifications', fontsize=17)\n",
    "_ = ax.set_ylabel('n listings', fontsize=17)\n",
    "ax.xaxis.set_tick_params(labelsize=14)\n",
    "ax.yaxis.set_tick_params(labelsize=14)\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "graph = sns.boxplot(\n",
    "    showfliers=False,\n",
    "    data=df_number_of_reviews,\n",
    "    palette=color)\n",
    "\n",
    "graph.axes.set_title(\"Reviews by Host-verifications\",fontsize=20)\n",
    "graph.set_xlabel(\"Number of Host Verifications\",fontsize=16)\n",
    "graph.set_ylabel(\"Number of Reviews\",fontsize=16)\n",
    "graph.tick_params(labelsize=12)\n",
    "\n",
    "# set labels\n",
    "_ = plt.xticks(df_number_of_reviews.columns)\n",
    "# reset figsize\n",
    "_ = plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly visible, a higher amount of _host-verifications_ generally yields higher _number-of-reviews_ which strengthens the claim of having more bookings. \n",
    "\n",
    "Next up is the same approach, focusing on the _availability_ of the listings. Furthermore, since the **sample size** of **0 _host-verifications_ listings is 2** and therefore much too low, it will be excluded from the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset splitup into n host verification lists\n",
    "\n",
    "availability = []\n",
    "\n",
    "for i in range(1, listings[\"host_verifications\"].max()):\n",
    "    host_ver = listings[listings[\"host_verifications\"] == i]\n",
    "    availability.append(pd.DataFrame(host_ver[\"availability_365\"].tolist(), columns=[i]))\n",
    "    \n",
    "df_availability = pd.concat(availability, ignore_index=True)\n",
    "    \n",
    "\n",
    "# Plotting\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "graph = sns.boxplot(\n",
    "    showfliers=False,\n",
    "    data=df_availability,\n",
    "    palette=color)\n",
    "\n",
    "graph.axes.set_title(\"Availability over 365 days by Host-verifications\",fontsize=20)\n",
    "graph.set_xlabel(\"Number of Host Verifications\",fontsize=16)\n",
    "graph.set_ylabel(\"Availability over 365 days\",fontsize=16)\n",
    "graph.tick_params(labelsize=12)\n",
    "\n",
    "# set labels\n",
    "_ = plt.xticks(range(10))\n",
    "# reset figsize\n",
    "_ = plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also this looks accurate when comparing the individual median values, even though the correlation doesn't look as clean as the one above. It definitely looks like that hosts with more than 2 _verifications_ get more bookings than any below. A further interesting finding is that for every number of _host-verifications_, one can find listings with almost an _availability_ of 365 days. This means, that there are several individual listings with an _availability_ of 365days/year.\n",
    "\n",
    "The median values are all situated pretty low, and the variation of the availability is very large for each bin.\n",
    "\n",
    "To get a better representation, excluding all listings with high availability throughout the year might help out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset splitup into n host verification lists\n",
    "\n",
    "availability = []\n",
    "availability_excl = 240\n",
    "\n",
    "for i in range(1, listings[\"host_verifications\"].max()):\n",
    "    host_ver = listings[listings[\"host_verifications\"] == i]\n",
    "    availability.append(pd.DataFrame(host_ver[host_ver[\"availability_365\"] < availability_excl][\"availability_365\"].tolist(), columns=[i]))\n",
    "    \n",
    "df_availability = pd.concat(availability, ignore_index=True)\n",
    "print()\n",
    "print(f'Excluded {total_all - df_availability.count().sum() - 2} listings from a total of {total_all-2} listings with availability > {availability_excl} for plotting.')\n",
    "    \n",
    "\n",
    "# Plotting\n",
    "plt.rcParams[\"figure.figsize\"] = (17,12)\n",
    "graph = sns.boxplot(\n",
    "    showfliers=False,\n",
    "    data=df_availability,\n",
    "    palette=color)\n",
    "\n",
    "\n",
    "graph.axes.set_title(\"Availability over 365 days by Host-verifications\",fontsize=20)\n",
    "graph.set_xlabel(\"Number of Host Verifications\",fontsize=16)\n",
    "graph.set_ylabel(\"Availability over 365 days\",fontsize=16)\n",
    "graph.tick_params(labelsize=12)\n",
    "\n",
    "# set labels\n",
    "_ = plt.xticks(range(10))\n",
    "# reset figsize\n",
    "_ = plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though a slight correlation is recognizable, having more than 2 verifications seems to be enough for hosts to get the most bookings. In contrast, a high _number-of-verifications_ yields more _reviews_. Considering the small sample sizes on both ends (0, 1, 9, 10 host verifications bins), the plots of those bins might not be that accurate.\n",
    "\n",
    "It might have something to do with special types of verifications that yield the most bookings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation: Price vs Amenities\n",
    "\n",
    "Further promising correlation found in the **correlation-heatmap**:\n",
    "- **positive correlation** for _amenities_ and _price_\n",
    "\n",
    "There is a strong-relationship between the _amount-of-amenities_ (e.g. extras) and their respective _prices_. Even though it might be an obvious correlation, the extend of its relationship remains to be showed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## exclude extraordinary price values\n",
    "new_listings = listings[listings[\"price\"] < 2000]\n",
    "\n",
    "# Plotting of scatter-plot for superhosts and non-superhosts\n",
    "_ = plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "_ = plt.xticks(fontsize = 15)\n",
    "_ = plt.yticks(fontsize = 15)\n",
    "_ = plt.xlabel(\"n number of amenities\", fontsize = 18)\n",
    "_ = plt.ylabel(\"price in \", fontsize = 18)\n",
    "_ = plt.title(f'Price / Amenities\\n', fontsize = 18)\n",
    "\n",
    "plot = plt.scatter('amenities', 'price', c = [\"red\"], data=new_listings)\n",
    "\n",
    "# Reset figsize for further plots\n",
    "_ = plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is hard to evaluate any correlation within this scatterplot, a visualization via a barplot and corresponding bins is tried below. It remains to be noted from the scatterplot above, that there might be **very low sample sizes for the latter bins**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'number_of_amenities':new_listings[\"amenities\"], 'price':new_listings[\"price\"]})\n",
    "df['number_of_amenities'] = pd.cut(df['number_of_amenities'], bins=range(0,81,10), labels=[f'{l}-{l+10}' for l in range(0,71,10)])\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(15,8))\n",
    "plot = sns.barplot(x='number_of_amenities', y='price', data=df, ax=ax, ci=None)\n",
    "\n",
    "_ = plt.xticks(fontsize = 15)\n",
    "_ = plt.yticks(fontsize = 15)\n",
    "_ = plt.xlabel(\"n number of amenities\", fontsize = 18)\n",
    "_ = plt.ylabel(\"avg. price in \", fontsize = 18)\n",
    "_ = plt.title(f'avg. Price / Amenities\\n', fontsize = 18)\n",
    "\n",
    "def change_width(ax, new_value) :\n",
    "    for patch in ax.patches :\n",
    "        current_width = patch.get_width()\n",
    "        diff = current_width - new_value\n",
    "\n",
    "        # we change the bar width\n",
    "        patch.set_width(new_value)\n",
    "\n",
    "        # we recenter the bar\n",
    "        patch.set_x(patch.get_x() + diff * .5)\n",
    "\n",
    "        \n",
    "change_width(ax, 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This perfectly illustrates the clean correlation between the _average prices_ for each bin of _n-amenities_. Interestingly, the _average price_ is lower than the previous for the last bin. Furthermore, **the _averaged price_ approximately doubles from 0-10 to 60-70 _amenities_**. The biggest increase of the price can be found for the pink bin.\n",
    "\n",
    "To visualize the sample-sizes for each bin, the distribution of listings over all bins is quickly checked below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.set(font_scale = 2)\n",
    "plot = sns.displot(x='number_of_amenities', data=df, height=7, aspect=3)\n",
    "_ = plot.set(xlabel='n number of amenities', ylabel=\"n listings\", title=f'Density Plot for each bin\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big differences for the last two bins might come from the fact that there are very low sample sizes for respective listings. All in all, the _averaged price_ increases monotonically and linear, over an increase in _amenities_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations with missing-reviews in listings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there has been a lot of missing values for the feature-block _ratings_, this chapter focuses on correlating it to any other attribute to possibly find any reasoning behind it.\n",
    "\n",
    "Firstly, a new feature in the dataset is going to be inserted, indicating whether a listing _has a rating_ or not to investigate if there are any obvious relations with other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert new boolean feature that indicates if the listing has a rating\n",
    "null_values_listings = listings\n",
    "null_values_listings[\"has_rating\"] = ~null_values_listings[\"review_scores_rating\"].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null_values_listings[null_values_listings[\"has_rating\"]==False].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, one can't see any correlation to other attributes.\n",
    "\n",
    "As it could be in correlation to being a superhost, its entries will be statistically checked below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_listings[null_values_listings[\"has_rating\"]==False][\"host_is_superhost\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can't be the case, since both, **superhosts** and **non-superhosts**, have null values present.\n",
    "\n",
    "To find out if there might be any other relevant correlations, calculating the **point biserial correlation** sounds reasonable, taking every continuos value present in the dataset and the binominal attribute _has_rating_. Missing entries will be replaced with mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"host_acceptance_rate\",\n",
    "         \"host_listings_count\",\n",
    "         \"host_total_listings_count\",\n",
    "         \"host_verifications\",\n",
    "         \"price\",\n",
    "         \"minimum_nights\",\n",
    "         \"maximum_nights\",\n",
    "         \"availability_365\",\n",
    "         \"calculated_host_listings_count\"]\n",
    "\n",
    "\n",
    "corr_list = []\n",
    "\n",
    "y = null_values_listings['has_rating'].astype(float)\n",
    "\n",
    "for column in columns:\n",
    "    # fill NaN values with mean\n",
    "    x=null_values_listings[column].fillna(null_values_listings[column].mean())\n",
    "    # apply pointbiserial\n",
    "    corr = stats.pointbiserialr(list(x), list(y))\n",
    "    corr_list.append(corr[0])\n",
    "\n",
    "print(f'Point Biserial Correlation\\nBinominal Feature = \\'has_rating\\'\\n')\n",
    "for i in range(len(columns)):\n",
    "    print(f'Continuous feature: {columns[i]}\\nCalculation: {corr_list[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values indicate that there is no strong relation between _has_ratings_ and the continuos features listed above. Unfortunately, this chapter is concluded without any findings about why there are missing ratings in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations: Districts / Price / Roomtype / Apartments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listings_without_outliers = listings without 2 most expensive apartments (over 6K price) \n",
    "listings_without_outliers = listings.copy()\n",
    "listings_without_outliers = listings_without_outliers[listings_without_outliers['price'] < 1500]\n",
    "\n",
    "# changing bedrooms to 1, 2, 3+\n",
    "listings_without_outliers['bedrooms'] = listings_without_outliers['bedrooms'].map(lambda x: '3+' if x >= 3 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive linking between neighbourhoods and districts\n",
    "Firstly, to get an overview the number of listings in each neighbourhood and each district. The result tells that the most listings are in the Centre District (1458 listings) followed by West District (1377) and the most linkings in specific neighbouhood is the Centrum-West neighbourhood (842) followed by De Baarsjes - Oud-West neighbourhood (788) from West District. \n",
    "\n",
    "Combining those two charts below with the use of interactivity to show clearly each neighbourhood in each district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brush = alt.selection_interval(encodings=['x'])\n",
    "\n",
    "chart = alt.Chart(listings).mark_bar().encode(\n",
    "    y = alt.Y('count()', axis=alt.Axis(title='Number of listings')),\n",
    "    tooltip=['count()', 'neighbourhood_district', 'average(price)'],\n",
    "    color=alt.condition(brush, 'neighbourhood_district', alt.value('lightgray'), legend=alt.Legend(title=\"Neighbourhood District:\"))\n",
    ").add_selection(\n",
    "    brush\n",
    ")\n",
    "\n",
    "chart.encode(x = alt.X('neighbourhood_district', sort='-y')) | chart.encode(x = alt.X('neighbourhood_cleansed', sort=alt.SortField('neighbourhood_district')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average price of each neighbourhood sorted by districts\n",
    "Secondly, to get an overview of average prices throughout each neighbourhood and district. As a result one can see  that on average obviously Centre District is the most expensive followed by Zuid and West district. And the most expensive neighbourhoods go to Centrum-Oost, Centrum-West and Zuid.\n",
    "\n",
    "Taking everything into account with numbers of listings one could find that the most listings and most expensive is the Centre District but that does not apply for second place. In second place the most listings are in West District but the second most expensive is Zuid District."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brush = alt.selection_interval(encodings=['x'])\n",
    "\n",
    "chart = alt.Chart(listings).mark_bar().encode(\n",
    "    y = alt.Y('average(price)', axis=alt.Axis(title='Average price in ')),\n",
    "    tooltip=['neighbourhood_district', 'neighbourhood_cleansed', 'average(price)', 'count()'],\n",
    "    color=alt.condition(brush, 'neighbourhood_district', alt.value('lightgray'), legend=alt.Legend(title=\"Neighbourhood District:\"))\n",
    ").add_selection(\n",
    "    brush\n",
    ")\n",
    "\n",
    "chart.encode(x = alt.X('neighbourhood_district', sort='y')) | chart.encode(x = alt.X('neighbourhood_cleansed', sort=alt.SortField('neighbourhood_district')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average price based on room type in each district\n",
    "The focus here is on room type. Let's pick the Centre District. Another factor of interest would be that average price of an apartment in the Centre is 237 , but there is considerable difference when choosing private room (around 134 on average).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(listings_without_outliers).mark_bar().encode(\n",
    "    x = alt.X('average(price)', axis=alt.Axis(title='Average price in ')),\n",
    "    y = alt.Y('room_type', axis=alt.Axis(title='')),\n",
    "    color = alt.Color('room_type', legend=alt.Legend(title=\"Room type:\")),\n",
    "    row = alt.Row('neighbourhood_district', title='Neighbourhood District'),\n",
    "    tooltip = ['neighbourhood_district', 'room_type', 'average(price)', 'count()']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average price based on room type in each district as a stacked histogram\n",
    "Another example would be Stacked histogram to additionally visualize average price of apartments in each district with certain room type.\n",
    "Using the normalize interactivity one could see the major difference in Zuidost District where average price of entire home/apartment is twice as much as hotel or private room."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(normalize=False)\n",
    "def stacked_hist(normalize):\n",
    "    if normalize == False:\n",
    "        return alt.Chart(listings).mark_bar().encode(\n",
    "            x = alt.X('average(price)', axis=alt.Axis(title='Average price in ')),\n",
    "            y = alt.Y('neighbourhood_district', axis=alt.Axis(title='Neighbourhood District')),\n",
    "            color = alt.Color('room_type', legend=alt.Legend(title=\"Room type:\")),\n",
    "            tooltip = ['neighbourhood_district', 'room_type', 'average(price)', 'count()']\n",
    "        )\n",
    "    else:\n",
    "        return alt.Chart(listings).mark_bar().encode(\n",
    "            x = alt.X('average(price)', stack='normalize', axis=alt.Axis(title='Average price in ')),\n",
    "            y = alt.Y('neighbourhood_district', axis=alt.Axis(title='Neighbourhood District')),\n",
    "            color = alt.Color('room_type', legend=alt.Legend(title=\"Room type:\")),\n",
    "            tooltip = ['neighbourhood_district', 'room_type', 'average(price)', 'count()']\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The popularity of 1, 2 and 3+ apartments\n",
    "Lets add another encoding which will be on the x axis and showing the superiority of one and two bedrooms apartments supplemented by a price. One can see where most airbnb offers are distributed by price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(listings_without_outliers).mark_point(opacity=0.4).encode(\n",
    "    x = alt.X('price', axis=alt.Axis(title='Price')),\n",
    "    color='bedrooms', \n",
    "    tooltip=['host_name', 'price', 'bedrooms']\n",
    ").properties(width=1500, height=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus here is to show that the most listings are 1 bedroom apartments. For better readability there is a Mark Type button which one could switch between point/circle/square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseChart = alt.Chart(listings_without_outliers);\n",
    "\n",
    "@widgets.interact(mark_type = widgets.Dropdown(\n",
    "    options = [('point', baseChart.mark_point()),\n",
    "                 ('circle', baseChart.mark_circle()), \n",
    "                 ('square', baseChart.mark_square()), \n",
    "                 ('tick', baseChart.mark_tick()), \n",
    "                 ('line', baseChart.mark_line())],\n",
    "    description = 'Mark Type:'))\n",
    "\n",
    "def show_plot(mark_type):\n",
    "    return mark_type.encode(\n",
    "        y = alt.Y('number_of_reviews', axis=alt.Axis(title='Number of reviews')),\n",
    "        x = alt.X('price', axis=alt.Axis(title='Price')),\n",
    "        color = alt.Color('bedrooms', legend=alt.Legend(title=\"Number of Bedrooms:\")),\n",
    "        tooltip = ['price', 'number_of_reviews', 'bedrooms']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Similar Items\n",
    "\n",
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now the dataset preserved its high dimensionality with 57 features. To increase the interpretability and plot them all in 2D, a dimensionality reduction technique which preserves the information is used, namely Principal Component Analysis. <br>\n",
    "Before applying PCA some more data manipulation is required, especially since our features consist both of numerical and categorical data. The latter are encoded in a series of distinct int64 via the method `panda.factorize()`, which transform them in <i>category</i>. A categorical variable takes on a limited, and usually fixed, number of possible values.<br> Furthermore all <i>Nan</i>, <i>inf</i> and missing cells must be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select all non-numeric data\n",
    "obj = listings.select_dtypes(exclude=\"number\")\n",
    "obj.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas.factorize() encode the array values in a series of distinct int64 \n",
    "\n",
    "listings['last_scraped'] = pd.Categorical(pd.factorize(listings['last_scraped'])[0])\n",
    "listings['host_name'] = pd.Categorical(pd.factorize(listings['host_name'])[0])\n",
    "listings['host_response_time'] = pd.Categorical(pd.factorize(listings['host_response_time'])[0])\n",
    "listings['host_is_superhost'] = pd.Categorical(pd.factorize(listings['host_is_superhost'])[0])\n",
    "listings['host_neighbourhood'] = pd.Categorical(pd.factorize(listings['host_neighbourhood'])[0])\n",
    "listings['host_has_profile_pic'] = pd.Categorical(pd.factorize(listings['host_has_profile_pic'])[0])\n",
    "listings['host_identity_verified'] = pd.Categorical(pd.factorize(listings['host_identity_verified'])[0])\n",
    "listings['neighbourhood_cleansed'] = pd.Categorical(pd.factorize(listings['neighbourhood_cleansed'])[0])\n",
    "listings['property_type'] = pd.Categorical(pd.factorize(listings['property_type'])[0])\n",
    "listings['room_type'] = pd.Categorical(pd.factorize(listings['room_type'])[0])\n",
    "listings['has_availability'] = pd.Categorical(pd.factorize(listings['has_availability'])[0])\n",
    "listings['instant_bookable'] = pd.Categorical(pd.factorize(listings['instant_bookable'])[0])\n",
    "listings['first_review'] = pd.Categorical(pd.factorize(listings['first_review'])[0])\n",
    "listings['last_review'] = pd.Categorical(pd.factorize(listings['last_review'])[0])\n",
    "listings['neighbourhood_district'] = pd.Categorical(pd.factorize(listings['neighbourhood_district'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop datetime64 features (not supported in pandas even if legit in numpy)\n",
    "date_feat = [\"last_review\", \"first_review\", \"host_since\", \"last_scraped\"]\n",
    "listings = listings.drop(date_feat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean the dataset from NaN, Inf, and missing cells.\n",
    "\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "\n",
    "listings = clean_dataset(listings.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= [i for i in listings.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we introduced new vector spaces with the encoding from categorical to numerical data, this often leads the variables to be in very different scale, which ultimately results in clustering even between uncorrelated datapoints. Thus, to avoid that, scaling is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the features\n",
    "# Separating out the features\n",
    "x = listings.loc[:, features].values\n",
    "x = listings.drop([\"host_acceptance_rate\",\"neighbourhood_cleansed\", \"price\"], axis=1)\n",
    "\n",
    "y = listings.loc[:,['host_acceptance_rate',\"neighbourhood_cleansed\",\"price\"]].values\n",
    "\n",
    "st_listings = StandardScaler().fit_transform(listings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(st_listings)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([principalDf, listings[['host_acceptance_rate','neighbourhood_cleansed', 'price']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.cumsum() #this should add up to 1 but doesn't???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final= clean_dataset(finalDf)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following an interactive plot is implemented.\n",
    "The user has free choice between 5 different clustering algorithms and the number of clusters.\n",
    "\n",
    "**K-Means**:\n",
    "* Probably the most common, it assigns datasamples to a cluster with the aim of minimizing the variance within each cluster.\n",
    "\n",
    "**BIRCH**:\n",
    "* Balanced Iterative Reducing and Clustering using Hierarchies uses a height-balanced tree data structure, it doesn't cluster directly the dataset but works with smaller partition/summary which are added to bigger ones, step by step.\n",
    "\n",
    "**Mini-Batch K-Means**:\n",
    "* Mini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.\n",
    "\n",
    "**Agglomerative Clustering**:\n",
    "* Agglomerative clustering involves merging examples until the desired number of clusters is achieved.\n",
    "\n",
    "**Gaussian mixture**:\n",
    "* Gaussian Mixture models assume that there are a certain number of Gaussian distributions, and each of these distributions represent a cluster. Hence, a Gaussian Mixture Model tends to group the data points belonging to a single distribution together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(n_clusters=(1,10), algorithm=[\"Kmeans\", \"Birch\", \"MiniBatchKMeans\", \"AgglomerativeClustering\", \"GaussianMixture\"]) \n",
    "def draw_plot(n_clusters, algorithm):\n",
    "    if algorithm ==\"Kmeans\":\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state = 102)\n",
    "        final[\"predict\"] = kmeans.fit_predict(final)\n",
    "    \n",
    "    elif algorithm == \"Birch\":\n",
    "        birch = Birch(threshold=0.01, n_clusters=n_clusters)\n",
    "        final[\"predict\"] = birch.fit_predict(final)\n",
    "    \n",
    "    elif algorithm == \"MiniBatchKMeans\":\n",
    "        miniBatchKMeans = MiniBatchKMeans(n_clusters=n_clusters)\n",
    "        final[\"predict\"] = miniBatchKMeans.fit_predict(final)\n",
    "    \n",
    "    elif algorithm == \"AgglomerativeClustering\":\n",
    "        agglomerativeClustering= AgglomerativeClustering(n_clusters=n_clusters)\n",
    "        final[\"predict\"] = agglomerativeClustering.fit_predict(final)\n",
    "        \n",
    "    elif algorithm == \"GaussianMixture\":\n",
    "        gaussianMixture = GaussianMixture(n_components=n_clusters)\n",
    "        final[\"predict\"] = gaussianMixture.fit_predict(final)\n",
    "   \n",
    "    #adjust for plotting    \n",
    "    selector = alt.selection_single(empty=\"all\", fields=['predict']) #sort the first plot\n",
    "    \n",
    "\n",
    "    base = alt.Chart(final).properties(\n",
    "        width=250,\n",
    "        height=250\n",
    "    ).add_selection(selector)\n",
    "\n",
    "    first = base.mark_point(filled=True, size=200).encode(\n",
    "        x='principal component 1',\n",
    "        y='principal component 2',\n",
    "        color=alt.condition(selector, 'predict:N', alt.value('lightgray')),\n",
    "    ).interactive()\n",
    "    \n",
    "    second = base.mark_point().encode(\n",
    "        x='price',\n",
    "        y=alt.Y('neighbourhood_cleansed', scale=alt.Scale(domain=(-15, 15))),\n",
    "        color=alt.Color('neighbourhood_cleansed:O')\n",
    "    ).transform_filter(\n",
    "        selector\n",
    "    ).interactive()\n",
    "    \n",
    "    \n",
    "    return first.properties(title='PCA') | second.properties(title='Price shown by neighbourhood')\n",
    "    #######################\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first graph all previous 57 features are summarized and plotted in 2D, they are not distinguishable any more, notwithstanding clusters mantains some differences with respect to neighbourhood and prices.<br>\n",
    "If we take the example with setting <b>n_clusters: 5</b> ald <b>algorithm: K-Means</b> we see that most of the datapoints are in (1)the red cluster (as well as the outtermost outlier), which entails all the 21 neighbourhoods and has a price range  0,~120 , (2) the blue cluster is one of the most dense group, and only goes from ~125 to 225,  (3)the light sky from ~225,370 , (4)the orange one instead entails ~ 350,630  but span only over a half of the neighbourhood, as well as the (5) the green, with only a few instances, from ~650 to 1,200 .<br> \n",
    "This continuous correspondence between cluster and price range is mantained with any setting of number of clusters; of course the more clusters the narrower their range.<br>\n",
    "Similar results are visible with <b>BIRCH</b>, <b>Minibatch-K-Means</b>, <b>Minibatch-K-Means</b> and <b>Agglomerative Clustering</b> which suggests a similarity in the operating of the algorithms.<br>\n",
    "Very different is the scenario when using <b>Gaussian Mixture</b>.<br>\n",
    "Taking into accounts the most outlier datapoint, which in K-Means belongs to the first red clusters, with Gaussian Mixture it not only denotes fewer neighbourhoods (only 10 out of 21) but the datapoints in this cluster cover the whole range of prices. <br>It is worth mentioning that all the previous algorithm put the two outter most outliers in different clusters, whereas in the Gaussian Mixture they pertain to the same one, even when setting the number of cluster to the max: 10. This difference is probably explained by the more flexible, probabilistic nature of this last algorithm, in contrast with the data-driven approach of the prior ones. <br> An optimal solution would be to use an hybrid approach to get the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Sources:<br>\n",
    "\n",
    "https://machinelearningmastery.com/clustering-algorithms-with-python/<br>\n",
    "https://analyticsindiamag.com/guide-to-birch-clustering-algorithmwith-python-codes/<br>\n",
    "https://www.analyticsvidhya.com/blog/2019/10/gaussian-mixture-models-clustering/<br>\n",
    "https://towardsdatascience.com/gaussian-mixture-models-vs-k-means-which-one-to-choose-62f2736025f0</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
